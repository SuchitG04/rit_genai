{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ML5LKLZ3uNxC"
      },
      "outputs": [],
      "source": [
        "from nltk.lm import Vocabulary\n",
        "import os\n",
        "import string\n",
        "from collections import defaultdict\n",
        "\n",
        "data_dir = \"data\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/sathya-pramodh/E2E-RAG.git\n",
        "! mv E2E-RAG/data ./"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkOsbtNE7F4a",
        "outputId": "f0225c46-421c-41a4-dc4e-fbf57f2fe736"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'E2E-RAG'...\n",
            "remote: Enumerating objects: 392, done.\u001b[K\n",
            "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 392 (delta 10), reused 30 (delta 6), pack-reused 356\u001b[K\n",
            "Receiving objects: 100% (392/392), 57.53 MiB | 31.88 MiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files = os.listdir(path=data_dir)\n",
        "all_lines = []\n",
        "for file in files:\n",
        "    with open(os.path.join(data_dir, file)) as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            all_lines.extend(line.strip().split('.'))"
      ],
      "metadata": {
        "id": "DU589I0_9Anj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_lines[:10], len(all_lines)"
      ],
      "metadata": {
        "id": "H5SgQExsO-Vc",
        "outputId": "8bdd4857-602c-4e7e-bab1-af4695283551",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Travian is now migrating its business intelligence systems to AWS using Amazon Redshift, which uses SQL to analyze structured and semistructured data across data warehouses, operational databases, and data lakes',\n",
              "  ' Using data analytics, Travian will be able to analyze player behavior in the game based on the 11 TB of data that it collects each month and make improvements',\n",
              "  ' “It used to be impossible for us to do this at this scale,” says Strathaus',\n",
              "  ' “We’re looking forward to using analytics to improve our games further on AWS',\n",
              "  '”',\n",
              "  'Français',\n",
              "  'Amazon Elastic Kubernetes Service (Amazon EKS) is a managed Kubernetes service to run Kubernetes in the AWS cloud and on-premises data centers',\n",
              "  '  Learn more\\xa0»',\n",
              "  'Travian needed a more stable service that could handle Kubernetes',\n",
              "  ' The studio was initially hesitant to use AWS because the offerings from AWS are so vast that Travian worried it would be overwhelming'],\n",
              " 82163)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this has to be run twice to eliminate all special characters and lower case everything\n",
        "# i don't know why that's the case\n",
        "for _ in range(7):\n",
        "    for i,line in enumerate(all_lines):\n",
        "        # https://stackoverflow.com/a/60725180/15368987\n",
        "        all_lines[i] = all_lines[i].strip().translate(str.maketrans('', '', string.punctuation)).lower()\n",
        "        if all_lines[i] == '' or \\\n",
        "        len(all_lines[i]) < 3:\n",
        "            all_lines.pop(i)"
      ],
      "metadata": {
        "id": "P9IFo-g3P_8C"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_lines[:10], len(all_lines)"
      ],
      "metadata": {
        "id": "9ZHpENHlQeCA",
        "outputId": "c75f44d9-2145-4553-df2e-c459758f04b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['travian is now migrating its business intelligence systems to aws using amazon redshift which uses sql to analyze structured and semistructured data across data warehouses operational databases and data lakes',\n",
              "  'using data analytics travian will be able to analyze player behavior in the game based on the 11 tb of data that it collects each month and make improvements',\n",
              "  '“it used to be impossible for us to do this at this scale” says strathaus',\n",
              "  '“we’re looking forward to using analytics to improve our games further on aws',\n",
              "  'français',\n",
              "  'amazon elastic kubernetes service amazon eks is a managed kubernetes service to run kubernetes in the aws cloud and onpremises data centers',\n",
              "  'learn more\\xa0»',\n",
              "  'travian needed a more stable service that could handle kubernetes',\n",
              "  'the studio was initially hesitant to use aws because the offerings from aws are so vast that travian worried it would be overwhelming',\n",
              "  'however as the need for reliability became paramount travian decided to give it a try'],\n",
              " 44823)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get each line. and also in each line split by '.' to get sentences. build vocab with appropriate min count in nltk\n",
        "#"
      ],
      "metadata": {
        "id": "1f1Rgk84IdEK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts = defaultdict(int)\n",
        "for line in all_lines:\n",
        "    for word in line.split():\n",
        "        word_counts[word] += 1"
      ],
      "metadata": {
        "id": "ABG_-fqqNC0U"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = [word for word, count in word_counts.items()]\n",
        "w2i = {w: i for i,w in enumerate(vocab)}\n",
        "i2w = {i: w for w,i in w2i.items()}\n",
        "len(vocab)"
      ],
      "metadata": {
        "id": "R6SUkqN4VeFf",
        "outputId": "b92fd9a3-f4c9-41ed-8f5d-259a6d06857f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20857"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2i[\"travian\"], i2w[0]"
      ],
      "metadata": {
        "id": "cAwu4ETRm43d",
        "outputId": "c1d9e3cf-8fed-43e0-d06a-af9cccc78283",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 'travian')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import tensor\n",
        "from torch import nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "qiOuPRYfn1V3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 200\n",
        "NUM_NEG_SAMPLES = 5\n",
        "NUM_SURROUNDING_WORDS = 2\n",
        "\n",
        "vocab_sz = len(vocab)"
      ],
      "metadata": {
        "id": "51n6kvoqXkgJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = []\n",
        "for sentence in all_lines[:5]:\n",
        "    sentence = sentence.split()\n",
        "    for word_idx, target_word in enumerate(sentence):\n",
        "        target_idx = w2i[target_word]\n",
        "        for j in range(max(0, word_idx - NUM_SURROUNDING_WORDS), min(len(sentence), word_idx + NUM_SURROUNDING_WORDS + 1)):\n",
        "            if word_idx == j: continue\n",
        "            # append surrounding word to labels\n",
        "            context_word = sentence[j]\n",
        "            context_idx = w2i[context_word]\n",
        "            pairs.append((target_idx, context_idx))\n"
      ],
      "metadata": {
        "id": "q3yNgRwZpyuB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs[:10]"
      ],
      "metadata": {
        "id": "2vwgtvkL0xzw",
        "outputId": "2efaed0e-936b-4519-980d-6e6f3c540418",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 1),\n",
              " (0, 2),\n",
              " (1, 0),\n",
              " (1, 2),\n",
              " (1, 3),\n",
              " (2, 0),\n",
              " (2, 1),\n",
              " (2, 3),\n",
              " (2, 4),\n",
              " (3, 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGram(nn.Module):\n",
        "    @staticmethod\n",
        "    def get_negative_samples(target_idx):\n",
        "        neg_samples = []\n",
        "        while len(neg_samples) < NUM_NEG_SAMPLES:\n",
        "            neg_sample = torch.randint(0, vocab_sz, (1, )).to('cuda')\n",
        "            if neg_sample != target_idx:\n",
        "                neg_samples.append(neg_sample)\n",
        "        return neg_samples\n",
        "\n",
        "    def __init__(self, vocab_sz, embedding_dim):\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.target_emb = nn.Embedding(vocab_sz, embedding_dim)\n",
        "        self.context_emb = nn.Embedding(vocab_sz, embedding_dim)\n",
        "\n",
        "    def forward(self, target_idx, context_idx, neg_samples):\n",
        "        # this is equivalent to * ing 1-hot-encoded vector with the emb/weight matrix\n",
        "        target_embedding = self.target_emb(target_idx)\n",
        "        context_embedding = self.context_emb(context_idx)\n",
        "        neg_embeddings = self.context_emb(neg_samples)\n",
        "\n",
        "        # (1, D) @ (D, 1) -> (1, 1)\n",
        "        pos_out = target_embedding[None] @ context_embedding[...,None]\n",
        "        pos_scores = pos_out.sigmoid()\n",
        "\n",
        "        # (1, NUM_NEG_SAMPLES, D) @ (1, D, 1) -> (1, NUM_NEG_SAMPLES, 1) -> (1, NUM_NEG_SAMPLES)\n",
        "        # neg_out = torch.bmm(neg_embeddings, target_embedding[...,None]).squeeze(-1)\n",
        "        neg_out = torch.einsum(\"nd,d -> n\", neg_embeddings, target_embedding)\n",
        "        neg_scores = neg_out.sigmoid()\n",
        "\n",
        "        return pos_scores, neg_scores"
      ],
      "metadata": {
        "id": "1w1NwDQ9n9Sq"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'\n",
        "model = SkipGram(vocab_sz, EMBEDDING_DIM).to(device)\n",
        "loss_fn = F.binary_cross_entropy\n",
        "lr = 0.1"
      ],
      "metadata": {
        "id": "nXtGo6SnLrog"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "    epoch_loss = 0\n",
        "    for t, c in pairs:\n",
        "        t, c = tensor(t).to(device), tensor(c).to(device)\n",
        "        neg_samples = model.get_negative_samples(t)\n",
        "        neg_samples = tensor(neg_samples).to(device)\n",
        "        p, n = model(t, c, neg_samples)\n",
        "\n",
        "        pl = torch.ones_like(p)\n",
        "        nl = torch.zeros_like(n)\n",
        "\n",
        "        loss = loss_fn(p, pl) + loss_fn(n, nl)\n",
        "        for p in model.parameters():\n",
        "            p.grad = None\n",
        "        loss.backward()\n",
        "\n",
        "        for p in model.parameters():\n",
        "            p.data += -lr * p.grad\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch: {i}; Loss: {epoch_loss}\")"
      ],
      "metadata": {
        "id": "AnV8vVUPAgfg",
        "outputId": "d06b1674-af6f-4ef5-fd32-fa4c4c0f4a38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0; Loss: 7007.018651020182\n",
            "Epoch: 1; Loss: 4672.861348517239\n",
            "Epoch: 2; Loss: 5224.104994235732\n",
            "Epoch: 3; Loss: 4922.869771795813\n",
            "Epoch: 4; Loss: 4082.163927370915\n",
            "Epoch: 5; Loss: 4871.11863450706\n",
            "Epoch: 6; Loss: 4468.280669493048\n",
            "Epoch: 7; Loss: 4671.96493857661\n",
            "Epoch: 8; Loss: 4149.169310898693\n",
            "Epoch: 9; Loss: 4273.095055247009\n",
            "Epoch: 10; Loss: 3970.177048623329\n",
            "Epoch: 11; Loss: 4013.0716511138526\n",
            "Epoch: 12; Loss: 4280.14077221106\n",
            "Epoch: 13; Loss: 3908.847448966556\n",
            "Epoch: 14; Loss: 4131.766737407539\n",
            "Epoch: 15; Loss: 3580.512126503221\n",
            "Epoch: 16; Loss: 3785.3665570202284\n",
            "Epoch: 17; Loss: 3753.145708022952\n",
            "Epoch: 18; Loss: 3570.6233106009854\n",
            "Epoch: 19; Loss: 3682.2023678884143\n",
            "Epoch: 20; Loss: 3627.688434967131\n",
            "Epoch: 21; Loss: 3765.872869040792\n",
            "Epoch: 22; Loss: 3342.011117795453\n",
            "Epoch: 23; Loss: 3549.887832555454\n",
            "Epoch: 24; Loss: 3187.9231333234056\n",
            "Epoch: 25; Loss: 3176.030073896327\n",
            "Epoch: 26; Loss: 3453.2587818381144\n",
            "Epoch: 27; Loss: 3250.655112879991\n",
            "Epoch: 28; Loss: 3140.826528161415\n",
            "Epoch: 29; Loss: 3211.0654438462225\n",
            "Epoch: 30; Loss: 3373.1421165339907\n",
            "Epoch: 31; Loss: 3069.211776921482\n",
            "Epoch: 32; Loss: 2534.430285103429\n",
            "Epoch: 33; Loss: 3048.626113345963\n",
            "Epoch: 34; Loss: 3131.8889122619294\n",
            "Epoch: 35; Loss: 3036.5398006277974\n",
            "Epoch: 36; Loss: 2729.3531773909926\n",
            "Epoch: 37; Loss: 2978.247768077068\n",
            "Epoch: 38; Loss: 2620.4497582861222\n",
            "Epoch: 39; Loss: 2690.8615652440567\n",
            "Epoch: 40; Loss: 2423.7119197202846\n",
            "Epoch: 41; Loss: 2318.4913021437824\n",
            "Epoch: 42; Loss: 2360.636349075008\n",
            "Epoch: 43; Loss: 2794.826029131975\n",
            "Epoch: 44; Loss: 2478.560064277015\n",
            "Epoch: 45; Loss: 2543.757472217083\n",
            "Epoch: 46; Loss: 2232.338195458986\n",
            "Epoch: 47; Loss: 2272.0788366161287\n",
            "Epoch: 48; Loss: 2457.1316425716504\n",
            "Epoch: 49; Loss: 2109.8564292953815\n",
            "Epoch: 50; Loss: 1927.1447795964777\n",
            "Epoch: 51; Loss: 2181.8704611007124\n",
            "Epoch: 52; Loss: 2105.4257949049497\n",
            "Epoch: 53; Loss: 2202.0150974329445\n",
            "Epoch: 54; Loss: 1896.426889700815\n",
            "Epoch: 55; Loss: 2201.092049192637\n",
            "Epoch: 56; Loss: 1953.835776566004\n",
            "Epoch: 57; Loss: 2254.324279272929\n",
            "Epoch: 58; Loss: 2035.6951586585492\n",
            "Epoch: 59; Loss: 1797.285728125018\n",
            "Epoch: 60; Loss: 1835.807347501628\n",
            "Epoch: 61; Loss: 1920.2753947228193\n",
            "Epoch: 62; Loss: 1590.8005844875588\n",
            "Epoch: 63; Loss: 1914.2907457945403\n",
            "Epoch: 64; Loss: 1781.7798716276884\n",
            "Epoch: 65; Loss: 1545.4796668244526\n",
            "Epoch: 66; Loss: 1821.1177693028003\n",
            "Epoch: 67; Loss: 1628.1692122695968\n",
            "Epoch: 68; Loss: 1492.8998229473364\n",
            "Epoch: 69; Loss: 1607.8823459696723\n",
            "Epoch: 70; Loss: 1662.218252405757\n",
            "Epoch: 71; Loss: 1465.130203391629\n",
            "Epoch: 72; Loss: 1691.0292309676297\n",
            "Epoch: 73; Loss: 1620.632919497706\n",
            "Epoch: 74; Loss: 1410.6507923910394\n",
            "Epoch: 75; Loss: 1422.1865478586406\n",
            "Epoch: 76; Loss: 1530.481692248024\n",
            "Epoch: 77; Loss: 1337.363948944956\n",
            "Epoch: 78; Loss: 1473.6549815372564\n",
            "Epoch: 79; Loss: 1279.2640377964126\n",
            "Epoch: 80; Loss: 1273.913806931232\n",
            "Epoch: 81; Loss: 1215.1419921743218\n",
            "Epoch: 82; Loss: 1331.7212366610765\n",
            "Epoch: 83; Loss: 1363.4031457300298\n",
            "Epoch: 84; Loss: 1205.1854520738125\n",
            "Epoch: 85; Loss: 1043.915480887983\n",
            "Epoch: 86; Loss: 1207.0973610676592\n",
            "Epoch: 87; Loss: 1107.301769492195\n",
            "Epoch: 88; Loss: 1157.0945024974644\n",
            "Epoch: 89; Loss: 1167.835618097335\n",
            "Epoch: 90; Loss: 997.2805555779487\n",
            "Epoch: 91; Loss: 1240.8021963587962\n",
            "Epoch: 92; Loss: 1233.2975799907\n",
            "Epoch: 93; Loss: 1013.3870667479932\n",
            "Epoch: 94; Loss: 1090.8577878531069\n",
            "Epoch: 95; Loss: 1035.352258052677\n",
            "Epoch: 96; Loss: 884.9379194276407\n",
            "Epoch: 97; Loss: 985.9517094958574\n",
            "Epoch: 98; Loss: 869.3957981434651\n",
            "Epoch: 99; Loss: 922.822047744412\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v1 = model.target_emb(tensor(w2i[\"athena\"], device=device))\n",
        "v2 = model.target_emb(tensor(w2i[\"aws\"], device=device))\n",
        "v3 = model.target_emb(tensor(w2i[\"fastapi\"], device=device))"
      ],
      "metadata": {
        "id": "mmnSj5yrErmO"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.dot(v1, v2)"
      ],
      "metadata": {
        "id": "EMjx4C6cErtu",
        "outputId": "adb493cf-b9b5-426d-ccf5-c48e1b9e2289",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(7.1017, device='cuda:0', grad_fn=<DotBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.dot(v1, v3)"
      ],
      "metadata": {
        "id": "qn5OpmxTIf23",
        "outputId": "f965b17d-44bb-48d5-879e-1d3523adb8fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-24.3139, device='cuda:0', grad_fn=<DotBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.dot(v2, v3)"
      ],
      "metadata": {
        "id": "JKS2nS6wIgFt",
        "outputId": "fcc58082-6671-496b-9d53-ac690d46567f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-1.7388, device='cuda:0', grad_fn=<DotBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d8fjSEivQXOU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}